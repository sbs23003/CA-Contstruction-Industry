{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbs23003/CA-Contstruction-Industry/blob/main/sentiment%20analysis\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install transformers\n",
        "!pip install spacy\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "9_Z0YZMjr-yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pip install command below if you don't already have the library\n",
        "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
        "\n",
        "# Run the below command if you don't already have Pandas\n",
        "# !pip install pandas\n",
        "\n",
        "# Imports\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Jki321YUgdyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting variables to be used below\n",
        "maxTweets = 200\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "tweets_list1 = []\n",
        "tweets_list2 = []\n",
        "\n",
        "\n",
        "search = ['housing market', 'construction sector', 'construction', 'construction costs', 'labour costs', 'housing prices', 'property developers', 'housing prices', 'construction prices', 'building developments']\n",
        "#Genious move\n",
        "queries = []\n",
        "for item in search:\n",
        "    queries.append(item + ' near:dublin until:2023-05-23 since:2022-01-01')\n",
        "    queries.append(item + ' near:Ireland until:2023-05-23 since:2022-01-01')\n",
        "\n",
        "#media = ['BBCNewsNI', 'BBCSpotlightNI', 'BBCOneNI', 'BBCnireland', 'bbcnipress', 'bbcradioulster', 'bbcnewsline', \n",
        "#         'BelTel', 'coolfm', 'QUBelfast', 'UlsterUni', 'utv']\n",
        "\n",
        "for q in queries:\n",
        "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'{q}').get_items()):\n",
        "        # include:nativeretweets\n",
        "        # quoted_tweet_id:1546434503316570113 lang:en\n",
        "        # conversation_id:1546434503316570113 lang:en\n",
        "        if i>20:  break\n",
        "        tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.location])\n",
        "        tweets_list2.append([tweet.url, tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.user.location, tweet.hashtags, \n",
        "                         tweet.replyCount, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.conversationId,\n",
        "                         tweet.lang, tweet.retweetedTweet, tweet.quotedTweet, tweet.mentionedUsers, tweet.inReplyToTweetId, tweet.inReplyToUser])\n"
      ],
      "metadata": {
        "id": "ALd2TNrNgd6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_list1\n"
      ],
      "metadata": {
        "id": "HFyYAyVjZ3eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe from the tweets list above\n",
        "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username', 'Location'])\n",
        "tweets_df2 = pd.DataFrame(tweets_list2, columns=['Url', 'Datetime', 'Tweet Id', 'Text', 'Username', 'Location', 'Hashtags', \n",
        "                                                 'Reply Count', 'Retweet Count', 'Like Count', 'Quote Count', 'Conv. Id',\n",
        "                                                 'Language', 'Retweeted Tweet', 'Quoted Tweet', 'Mentioned Users', 'Replied Tweet', \n",
        "                                                 'Replied User'])\n",
        "# Display first 5 entries from dataframe\n",
        "tweets_df1.head()\n",
        "tweets_df2.head()"
      ],
      "metadata": {
        "id": "ZoHXM4LpiJ4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export dataframe into a CSV\n",
        "tweets_df1.to_csv('text-query-tweets.csv', sep=',', index=False)\n",
        "tweets_df2.to_csv('text-query-tweets-detailed.csv', sep=',', index=False)"
      ],
      "metadata": {
        "id": "JeHDk0jqj_uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df1"
      ],
      "metadata": {
        "id": "DCA_WnDulb-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from tqdm.notebook import tqdm\n",
        "import emoji\n",
        "import transformers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from google.colab import drive\n",
        "import spacy\n",
        "from wordcloud import WordCloud\n",
        "from langdetect import detect\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "JScOKzjYr7Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt') \n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "XNlBO5vHtYV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "raw_df = pd.read_csv('/content/text-query-tweets.csv')\n",
        "\n",
        "raw_df"
      ],
      "metadata": {
        "id": "o_QyvRZntas7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# locate column with texts\n",
        "text_data_raw = raw_df['Text'].values"
      ],
      "metadata": {
        "id": "HGBhAoo0twIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_data_raw)"
      ],
      "metadata": {
        "id": "MZqlFGNgt_mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Removing non-English tweets"
      ],
      "metadata": {
        "id": "2V7lQUJjuGQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "languages = []\n",
        "text_data = []\n",
        "en_count = 0\n",
        "for text in text_data_raw:\n",
        "  if text != '':\n",
        "    try:\n",
        "      language = detect(text)\n",
        "      if language == 'en':\n",
        "        en_count = en_count + 1\n",
        "        text_data.append(text)\n",
        "    except:\n",
        "      continue\n",
        "    languages.append(language)"
      ],
      "metadata": {
        "id": "ecBL9wYPuPLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(languages, return_counts = True)"
      ],
      "metadata": {
        "id": "GTZSaskpuT11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_count)"
      ],
      "metadata": {
        "id": "1fClyBZnuV8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store English tweets as new df\n",
        "clean_df = pd.DataFrame(text_data, columns = ['Text'])\n",
        "\n",
        "clean_df"
      ],
      "metadata": {
        "id": "SVqpvR7iuZ4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing duplicated\n"
      ],
      "metadata": {
        "id": "aA7wB2Azufoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = clean_df.drop_duplicates(keep='first')\n",
        "\n",
        "clean_df"
      ],
      "metadata": {
        "id": "Z8Z8DaMtujmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle dataframe rows\n",
        "df = clean_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "Lw7TsA8AurRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to download the dataset\n",
        "\n",
        "!wget \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\" -O data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "id": "xn9_i3MTwopK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',\n",
        "                       names=['polarity', 'id', 'date', 'query', 'user', 'text'],\n",
        "                       encoding='latin-1')\n",
        "\n",
        "label_df"
      ],
      "metadata": {
        "id": "tD66J7Bfwzo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label_df = label_df.drop(columns=['id', 'date', 'query', 'user'])\n",
        "label_df"
      ],
      "metadata": {
        "id": "hf5772r0xgWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = label_df.copy()\n",
        "c = c.dropna()"
      ],
      "metadata": {
        "id": "yLJQXkmLZ3LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df = c"
      ],
      "metadata": {
        "id": "1E4Pn5WCZ-Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df['polarity'].unique()"
      ],
      "metadata": {
        "id": "eSefHDpf0W24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji_cleaning(text):\n",
        "    \n",
        "  # Change emoji to text\n",
        "  text = emoji.demojize(text).replace(\":\", \" \")\n",
        "  \n",
        "  # Delete repeated emoji\n",
        "  tokenizer = text.split()\n",
        "  repeated_list = []\n",
        "  \n",
        "  for word in tokenizer:\n",
        "      if word not in repeated_list:\n",
        "          repeated_list.append(word)\n",
        "  \n",
        "  text = ' '.join(text for text in repeated_list)\n",
        "  text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n",
        "  return text\n",
        "    \n",
        "\n",
        "def clean_urls(review):\n",
        "    review = review.split()\n",
        "    review = ' '.join([word for word in review if not re.match('^http', word)])\n",
        "    return review\n",
        "\n",
        "def decontracted(text):\n",
        "    text = re.sub(r\"won\\’t\", \"will not\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"can\\’t\", \"can not\", text)\n",
        "    text = re.sub(r\"n\\’t\", \" not\", text)\n",
        "    text = re.sub(r\"\\’re\", \" are\", text)\n",
        "    text = re.sub(r\"it\\'s\", \"it is\", text)\n",
        "    text = re.sub(r\"\\’d\", \" would\", text)\n",
        "    text = re.sub(r\"\\’ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\’t\", \" not\", text)\n",
        "    text = re.sub(r\"\\’ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\’m\", \" am\", text)\n",
        "\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\’re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\’d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\’ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\’t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\’ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    text = re.sub(r\"\\’m\", \" am\", text)\n",
        "    text = re.sub(r\"\\“\", \"\", text)\n",
        "    text = re.sub(r\"\\”\", \"\", text)\n",
        "    text = re.sub(r\"\\…\", \"\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n",
        "    text = re.sub(r'[^a-zA-Z ]+', ' ', text)\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    text = re.sub(r'pic.twitter\\S+', ' ', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n",
        "\n",
        "stop_words = stopwords.words('english') \n",
        "stop_words.remove('not') \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def data_preprocessing(review):\n",
        "    \n",
        "  # data cleaning\n",
        "    review = re.sub(re.compile('<.*?>'), '', review) #removing html tags\n",
        "    review =  re.sub('[^A-Za-z0-9]+', ' ', review) #taking only words\n",
        "  \n",
        "  # lowercase\n",
        "    review = review.lower()\n",
        "  \n",
        "  # tokenization\n",
        "    tokens = nltk.word_tokenize(review) # converts review to tokens\n",
        "  \n",
        "  # stop_words removal\n",
        "    review = [word for word in tokens if word not in stop_words] #removing stop words\n",
        "  \n",
        "  # lemmatization\n",
        "    review = [lemmatizer.lemmatize(word) for word in review]\n",
        "  \n",
        "  # join words in preprocessed review\n",
        "    review = ' '.join(review)\n",
        "    return review"
      ],
      "metadata": {
        "id": "2yUYM_xk1h0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datadata = df.copy()\n",
        "df['clean_text'] = df['Text'].apply(clean_urls).apply(clean_text).apply(emoji_cleaning).apply(decontracted)\n",
        "#.apply(correct_spellings)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vkKjB_2B2HXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df['clean_text'] = label_df['text'].apply(clean_urls).apply(clean_text).apply(emoji_cleaning).apply(decontracted)\n",
        "#.apply(correct_spellings)\n",
        "     "
      ],
      "metadata": {
        "id": "pT_1LtOO1HfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['clean_text'].apply(lambda review: data_preprocessing(review))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "gzFZ_ObE2V5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df['clean_text'] = label_df['clean_text'].apply(lambda review: data_preprocessing(review))\n",
        "\n",
        "label_df"
      ],
      "metadata": {
        "id": "NVqcVW6B2b3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_word_counts = df['Text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "raw_word_counts.describe()"
      ],
      "metadata": {
        "id": "rOUlLifq4Vu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_word_counts = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "clean_word_counts.describe()"
      ],
      "metadata": {
        "id": "1Oipq5Pm4bUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp"
      ],
      "metadata": {
        "id": "cu4ip6t64ey-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clean_text_data = df['clean_text'].values"
      ],
      "metadata": {
        "id": "Fr5WUlY14j1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(clean_text_data)"
      ],
      "metadata": {
        "id": "2adPBlJu4nEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = ''\n",
        "for text in clean_text_data:\n",
        "  texts += ' ' + text\n",
        "  "
      ],
      "metadata": {
        "id": "neaeBCai4oxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "len(texts)"
      ],
      "metadata": {
        "id": "dL4Ewkvv4rPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokens = nltk.tokenize.word_tokenize(texts)"
      ],
      "metadata": {
        "id": "2BAHYqUQ4uwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "id": "tDXDufRR4w63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency = nltk.FreqDist(tokens)\n",
        "most_common = frequency.most_common()\n",
        "\n",
        "# most_common\n",
        "print(most_common[0:20])"
      ],
      "metadata": {
        "id": "30u3TVMD4zOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud = WordCloud()\n",
        "cloud = cloud.generate(texts)\n",
        "plt.figure(figsize=(15,15))\n",
        "plt.imshow(cloud)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "OvG8rbmV40iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train data"
      ],
      "metadata": {
        "id": "oLm-nB8s7kGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET\n",
        "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "# TEXT CLENAING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# SENTIMENT\n",
        "POSITIVE = \"POSITIVE\"\n",
        "NEGATIVE = \"NEGATIVE\"\n",
        "NEUTRAL = \"NEUTRAL\"\n",
        "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
        "\n",
        "# EXPORT\n",
        "KERAS_MODEL = \"model.h5\"\n",
        "WORD2VEC_MODEL = \"model.w2v\"\n",
        "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
        "ENCODER_MODEL = \"encoder.pkl\""
      ],
      "metadata": {
        "id": "dxqkZw6TH_ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, df_test = train_test_split(label_df, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "metadata": {
        "id": "Ef5XqHjS7fHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "documents = [_text.split() for _text in df_train.text] "
      ],
      "metadata": {
        "id": "9GoJazwjIHV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=W2V_MIN_COUNT, workers=8)"
      ],
      "metadata": {
        "id": "p8H7CaouIKsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.build_vocab(documents)"
      ],
      "metadata": {
        "id": "7Tlg1pvmJ422"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = w2v_model.wv\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "metadata": {
        "id": "qQRgOodS7s2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
      ],
      "metadata": {
        "id": "9lHgRRveLEA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(\"love\")"
      ],
      "metadata": {
        "id": "rVLg-KvYPATq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize Text"
      ],
      "metadata": {
        "id": "B-ZlFbwgQBjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train.text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ],
      "metadata": {
        "id": "MX1pWrBtQDSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "ReX2LlhXQNNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns\n"
      ],
      "metadata": {
        "id": "kcMcC2OqTBC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df_train.polarity.unique().tolist()\n",
        "labels"
      ],
      "metadata": {
        "id": "eay-OolvQdEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "WtgglzooTuya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train.polarity.tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train.polarity.tolist())\n",
        "y_test = encoder.transform(df_test.polarity.tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ],
      "metadata": {
        "id": "npAIj4JRTkKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ],
      "metadata": {
        "id": "6V94Z-75SBws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
      ],
      "metadata": {
        "id": "knmqbVnzSE_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
      ],
      "metadata": {
        "id": "9ZEaEGBUST0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "m36P_d_RSG0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ZG3xVkTySMI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]"
      ],
      "metadata": {
        "id": "vLv6nMG8Sm17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.1,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "ZhLeuVDNSpvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "metadata": {
        "id": "FuaYu2OTdM5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history['acc']"
      ],
      "metadata": {
        "id": "1aNyUtQodd9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sentiment(score, include_neutral=True):\n",
        "    if include_neutral:        \n",
        "        label = NEUTRAL\n",
        "        if score <= SENTIMENT_THRESHOLDS[0]:\n",
        "            label = NEGATIVE\n",
        "        elif score >= SENTIMENT_THRESHOLDS[1]:\n",
        "            label = POSITIVE\n",
        "\n",
        "        return label\n",
        "    else:\n",
        "        return NEGATIVE if score < 0.5 else POSITIVE"
      ],
      "metadata": {
        "id": "ol5zR-XVeA2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def predict(text, include_neutral=True):\n",
        "    start_at = time.time()\n",
        "    # Tokenize text\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
        "    # Predict\n",
        "    score = model.predict([x_test])[0]\n",
        "    # Decode sentiment\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\n",
        "\n",
        "    return {\"label\": label, \"score\": float(score),\n",
        "       \"elapsed_time\": time.time()-start_at}  "
      ],
      "metadata": {
        "id": "t15gMX_jeEZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"good love wonderful im happy\")"
      ],
      "metadata": {
        "id": "g_lyZPTdeIU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"i don't know what i'm doing\")"
      ],
      "metadata": {
        "id": "zVihNk0-fEg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "y_pred_1d = []\n",
        "y_test_1d = list(df_test.polarity)\n",
        "scores = model.predict(x_test, verbose=1, batch_size=8000)\n",
        "y_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]"
      ],
      "metadata": {
        "id": "rKp7I1G7fOvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=30)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n",
        "    plt.yticks(tick_marks, classes, fontsize=22)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label', fontsize=25)\n",
        "    plt.xlabel('Predicted label', fontsize=25)"
      ],
      "metadata": {
        "id": "CqjlhzotfX87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "FMiM885xfp0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\n",
        "plt.figure(figsize=(12,12))\n",
        "plot_confusion_matrix(cnf_matrix, classes=df_train.polarity.unique(), title=\"Confusion matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MuhU8SdKffqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        " \n",
        "epochs = range(len(acc))\n",
        " \n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        " \n",
        "plt.figure()\n",
        " \n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tqq8peHudSkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_df"
      ],
      "metadata": {
        "id": "qZ8Pe06U7wFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "fkWAWd9I70Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split and train"
      ],
      "metadata": {
        "id": "-Hwq-9DiHmAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# labeled train data\n",
        "train_label_file_name = 'biden_tweets_labeled_train.csv'\n",
        "  \n",
        "train_df.to_csv(train_label_file_name)"
      ],
      "metadata": {
        "id": "_LkyrDMw74Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# labeled test data\n",
        "test_label_file_name = 'biden_tweets_labeled_test.csv'\n",
        "  \n",
        "test_df.to_csv(test_label_file_name)"
      ],
      "metadata": {
        "id": "f5ayd9IV77rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "nOEhRiJcCJla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re"
      ],
      "metadata": {
        "id": "_f31Uwl0CBo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(label_df['clean_text'].values)\n",
        "X = tokenizer.texts_to_sequences(label_df['clean_text'].values)\n",
        "X = pad_sequences(X)"
      ],
      "metadata": {
        "id": "3GxUtJ-tC66O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "UvN5FM6JC6zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(label_df['polarity']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "LCcpcQEeDixH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"
      ],
      "metadata": {
        "id": "JLJMTZUgDq5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "metadata": {
        "id": "S2JHdr1gDvHC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}